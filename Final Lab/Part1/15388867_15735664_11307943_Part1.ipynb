{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://miro.medium.com/v2/resize:fit:1250/format:webp/1*QgI1t-7yJApi4vQigFgsLQ.jpeg\" width=25% > </center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center> \n",
    "    <font size=\"6\">Final Lab (Part 1): Keypoint Detection, Bag of Visual Words and Image Classification</font>\n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\">Computer Vision 1 University of Amsterdam</font> \n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\">Due 23:59PM, October 18, 2024 (Amsterdam time)</font> \n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\"><b>TA's: Vlad, Matey & Antonios</b></font>\n",
    "</center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "***\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center>\n",
    "\n",
    "Student1 ID:  15388867\\\n",
    "Student1 Name: Jose Garcia\n",
    "\n",
    "Student2 ID: 15735664\\\n",
    "Student2 Name: Lisanne Wallaard\n",
    "\n",
    "Student3 ID: 11307943\\\n",
    "Student3 Name: Julio Smidi\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **General Guidelines**\n",
    "\n",
    "Your code must be handed in this Jupyter notebook, renamed to **StudentID1_StudentID2_StudentID3.ipynb** before the deadline by submitting it to the Canvas Final Lab (Part 1) Assignment. Please also fill out your names and IDs above.\n",
    "\n",
    "For full credit, make sure your notebook follows these guidelines:\n",
    "\n",
    "- Please express your thoughts **concisely**. The number of words does not necessarily correlate with how well you understand the concepts.\n",
    "- Understand the problem as much as you can. When answering a question, provide evidence (qualitative and/or quantitative results, references to papers, figures, etc.) to support your arguments. Not everything might be explicitly asked for, so think about what might strengthen your arguments to make the notebook self-contained and complete.\n",
    "- Tables and figures must be accompanied by a **brief** description. Add a number, a title, and, if applicable, the name and unit of variables in a table, and name and unit of axes and legends in a figure.\n",
    "\n",
    "**Late submissions are not allowed.** Assignments submitted after the strict deadline will not be graded. In case of submission conflicts, TAs’ system clock is taken as reference. We strongly recommend submitting well in advance to avoid last-minute system failure issues.\n",
    "\n",
    "**Environment:** Since this is a project-based assignment, you are free to use any feature descriptor and machine learning tools (e.g., K-means, SVM). You should use Python for your implementation. You are free to use any Python library for this assignment, but make sure to provide a conda environment file!\n",
    "\n",
    "**Plagiarism Note:** Keep in mind that plagiarism (submitted materials which are not your work) is a serious offense and any misconduct will be addressed according to university regulations. This includes using generative tools such as ChatGPT.\n",
    "\n",
    "**Ensure that you save all results/answers to the questions (even if you reuse some code).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Report Preparation**\n",
    "\n",
    "Your tasks include the following:\n",
    "\n",
    "1. **Report Preparation:** For both parts of the final project, students are expected to prepare a report. The report should include all details on implementation approaches, analysis of results for different settings, and visualizations illustrating experiments and performance of your implementation. Grading will be based on the report, so it should be as self-contained as possible. If the report contains faulty results or ambiguities, TAs can refer to your code for clarification. Only section 10 of this notebook should **not** be included in the report.\n",
    "\n",
    "2. **Explanation of Results:** Do not just provide numbers without explanation. Discuss different settings to show your understanding of the material and processes involved.\n",
    "\n",
    "3. **Quantitative Evaluation:** For quantitative evaluation, you are expected to provide the results based on the mAP (mean Average Precision) metric. You should report the mAP for each experimental setup. \n",
    "\n",
    "4. **Qualitative Evaluation:** For qualitative evaluation, you are expected to visualize the top-5 and bottom-5 ranked test images (based on classifier confidence for the target class) per setup. Provide a figure for each experimental setup Visual elements such as charts, graphs, and plots are always useful. Keep this in mind while writing your reports.\n",
    "\n",
    "5. **Aim:** Understand the basic Image Classification pipeline using a traditional Bag of Visual Words method.\n",
    "\n",
    "6. **Working on Assignments:** Students should work in assigned groups for **two** weeks. Any questions can be discussed on ED.\n",
    "\n",
    "    - **Submission:** Submit your source code and report together in a zip file (`ID1_ID2_ID3_part1.zip`). The report should be a maximum of 10 pages (single-column, including tables and figures, excluding references and appendix). Express thoughts concisely. Tables and figures must be accompanied by a description. Number them and, if applicable, name variables in tables, and label axes in figures.\n",
    "\n",
    "7. **Hyperparameter Search:** In your experiments, remember to perform a hyperparameter search to find the optimal settings for your classifier. Clearly document the search process, the parameters you explored, and how they influenced the performance of your model.\n",
    "\n",
    "8. **Format and Testing:** The report should be in **PDF format**, and the code in **.ipynb format**. Test that all functionality works as expected in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview**\n",
    "\n",
    "- [Section 1: Data Preparation (0 points)](#section-1)\n",
    "- [Section 2: Keypoint Detection and Feature Extraction (3 points)](#section-2)\n",
    "- [Section 3: Building the Visual Vocabulary (3 points)](#section-3)\n",
    "- [Section 4: Encoding Train Image Features (3 points)](#section-4)\n",
    "- [Section 5: Visualizing the Bag of Visual Words for Each Class (3 points)](#section-5)\n",
    "- [Section 6: Encoding Test Image Features (0 points)](#section-6)\n",
    "- [Section 7: Training the Classifiers (5 points)](#section-7)\n",
    "- [Section 8: Evaluating the Classifiers (12 points)](#section-8)\n",
    "- [Section 9: Hyperparameter Search (16 points)](#section-9)\n",
    "- [Section 10: Using CLIP for Image Classification (5 points)](#section-10)\n",
    "- [Section X: Individual Contribution Report (Mandatory)](#section-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Section 1: Data Preparation (0 points)**\n",
    "\n",
    "The goal of this lab is to implement an image classification system that can identify objects from a given set of classes. You will perform a 5-class image classification using a bag-of-words approach ([reference](http://www.robots.ox.ac.uk/~az/icvss08_az_bow.pdf)). The classes for this task are:\n",
    "\n",
    "1. **Frog**\n",
    "2. **Automobile**\n",
    "3. **Bird**\n",
    "4. **Cat**\n",
    "5. **Deer**\n",
    "\n",
    "The [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) will be used for this task. This dataset contains 32x32 pixel RGB images, divided into sub-directories with 5000 training images and 1000 test images for each class.\n",
    "\n",
    "The dataset will be automatically downloaded using the code provided in this notebook. You will need to perform training on the training set, which will later be divided into two subsets: one for building the visual vocabulary and another for training the classifier. Using more samples for training generally results in better performance. However, if computational resources are limited, you may use fewer training images to save time, as long as at least 500 images per class are included.\n",
    "\n",
    "The system must be tested using the specified subset of test images. Use all 1000 test images (per class) to observe the full performance of the model. Ensure that test images are excluded from training to maintain a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "# Define total train and test sizes\n",
    "total_train_size = 5000  # Default value for total training images\n",
    "total_test_size = 1000   # Default value for total test images\n",
    "\n",
    "# Define batch sizes for DataLoader\n",
    "train_batch_size = total_train_size\n",
    "test_batch_size = total_test_size\n",
    "\n",
    "# Define the number of Visual Words\n",
    "num_of_visual_words = 1000  # Default value for number of visual words\n",
    "\n",
    "# Number of classes\n",
    "num_classes = 5\n",
    "\n",
    "# Compute images per class for training and testing\n",
    "images_per_class_train = total_train_size // num_classes  # e.g., 5000 // 5 = 1000 per class\n",
    "images_per_class_test = total_test_size // num_classes    # e.g., 1000 // 5 = 200 per class\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  \n",
    "])\n",
    "\n",
    "# Define the class indices for the 5 selected classes: frog, automobile, bird, cat, and deer\n",
    "selected_classes = [6, 1, 2, 3, 4]  # 6: frog, 1: automobile, 2: bird, 3: cat, 4: deer\n",
    "class_to_label = {orig_class: new_label for new_label, orig_class in enumerate(selected_classes)}\n",
    "\n",
    "# Load the CIFAR-10 training set\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Function to filter and remap dataset\n",
    "def filter_dataset(dataset, images_per_class, selected_classes, class_to_label):\n",
    "    selected_indices = []\n",
    "    class_counts = {class_idx: 0 for class_idx in selected_classes}\n",
    "    remapped_labels = []\n",
    "\n",
    "    for idx, (image, label) in enumerate(dataset):\n",
    "        if label in selected_classes and class_counts[label] < images_per_class:\n",
    "            selected_indices.append(idx)\n",
    "            remapped_labels.append(class_to_label[label])\n",
    "            class_counts[label] += 1\n",
    "\n",
    "            # Stop if we have enough samples for each class\n",
    "            if all(count >= images_per_class for count in class_counts.values()):\n",
    "                break\n",
    "\n",
    "    filtered_dataset = Subset(dataset, selected_indices)\n",
    "    return filtered_dataset, remapped_labels\n",
    "\n",
    "# Filter and remap training set\n",
    "filtered_train_set, train_mapped_labels = filter_dataset(train_set, images_per_class_train, selected_classes, class_to_label)\n",
    "\n",
    "# Load the CIFAR-10 test set\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Filter and remap test set\n",
    "filtered_test_set, test_mapped_labels = filter_dataset(test_set, images_per_class_test, selected_classes, class_to_label)\n",
    "\n",
    "# Create data loaders for the filtered datasets\n",
    "train_data_loader = DataLoader(filtered_train_set, batch_size=train_batch_size, shuffle=False)\n",
    "test_data_loader = DataLoader(filtered_test_set, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "# Extract all training data and remapped labels\n",
    "train_images, _ = next(iter(train_data_loader))\n",
    "train_labels = torch.tensor(train_mapped_labels)\n",
    "\n",
    "train_images = train_images.permute(0, 2, 3, 1)\n",
    "print(f\"Filtered train data: {train_images.shape}\")\n",
    "print(f\"Filtered train labels: {train_labels.shape}\")\n",
    "\n",
    "# Extract all test data and remapped labels\n",
    "test_images, _ = next(iter(test_data_loader))\n",
    "test_labels = torch.tensor(test_mapped_labels)\n",
    "\n",
    "test_images = test_images.permute(0, 2, 3, 1)\n",
    "print(f\"Filtered test data: {test_images.shape}\")\n",
    "print(f\"Filtered test labels: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-2\"></a>\n",
    "### **Section 2: Keypoint Detection and Feature Extraction (3 points)**\n",
    "\n",
    "In this section, you will work on detecting keypoints and extracting features from the dataset. Your task is to use **two different feature extraction techniques** to identify keypoints in the images. Visualize two images from each of the five classes (Frog, Automobile, Bird, Cat, Deer) for both feature extraction techniques. For each image, draw circles around the detected keypoints that represent their size.\n",
    "\n",
    "This step is essential to understand how different feature extractors behave across various classes, setting the foundation for further analysis and classification in later steps.\n",
    "\n",
    "**Hint:** You can use the OpenCV library to detect keypoints and extract features. You can also upscale the images to improve the visualization of the keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_tensor(tensor):\n",
    "    \"\"\"\n",
    "    Given a tensor that ranges from -1 to 1, scale it so it ranges from to 0 - 255\n",
    "    \"\"\"\n",
    "    return np.interp(tensor.numpy(), (-1, 1), (0, 255)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Lecture 8, options for detecting keypoints and extracting features: SIFT, ORB, SURF, FAST, etc.\n",
    "# We will use SIFT and ORB for this assignment\n",
    "def get_image_keypoints_and_descriptors(img, method='sift'):\n",
    "    \"\"\"\n",
    "    Given an input image, find and return its keypoints and features.\n",
    "\n",
    "    Args:\n",
    "        image: The image in grayscale\n",
    "\n",
    "    Returns:\n",
    "        keypoints_image: Keypoints detected in the first image\n",
    "    \"\"\"\n",
    "\n",
    "    # turn the image to grayscale\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # create the extractor object\n",
    "    if method == 'sift':\n",
    "        extractor = cv2.SIFT_create()\n",
    "    elif method == 'orb':\n",
    "        extractor = cv2.ORB_create()\n",
    "\n",
    "    img_keypoints, img_descriptors = extractor.detectAndCompute(img_gray, None)\n",
    "\n",
    "    return img_keypoints, img_descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first two images of each class\n",
    "class_names = ['Frog', 'Automobile', 'Bird', 'Cat', 'Deer']\n",
    "\n",
    "fig, axes = plt.subplots(num_classes, 4, figsize=(15, 15))\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "\n",
    "    class_images = train_images[train_labels == i][:2]\n",
    "\n",
    "    for n, image in enumerate(class_images):\n",
    "\n",
    "        image = cv2.resize(fix_tensor(image), (128, 128))\n",
    "        \n",
    "        sift_keypoints, _ = get_image_keypoints_and_descriptors(image, 'sift')\n",
    "        orb_keypoints, _ = get_image_keypoints_and_descriptors(image, 'orb')\n",
    "\n",
    "        # Draw the keypoints on the image\n",
    "        image_w_sift_keypoints = cv2.drawKeypoints(image, sift_keypoints, 0, (255,0,0), cv2.DRAW_MATCHES_FLAGS_DEFAULT)\n",
    "        image_w_orb_keypoints = cv2.drawKeypoints(image, orb_keypoints, 0, (255,0,0), cv2.DRAW_MATCHES_FLAGS_DEFAULT)\n",
    "\n",
    "        x_pos = n * 2\n",
    "\n",
    "        axes[i, x_pos].imshow(image_w_sift_keypoints)\n",
    "        axes[i, x_pos].set_title(f\"{class_name} #{n+1}: SIFT Keypoints\")\n",
    "        axes[i, x_pos].axis('off')\n",
    "\n",
    "        axes[i, x_pos + 1].imshow(image_w_orb_keypoints)\n",
    "        axes[i, x_pos + 1].set_title(f\"{class_name} #{n+1}: ORB Keypoints\")\n",
    "        axes[i, x_pos + 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3\"></a>\n",
    "### **Section 3: Building the Visual Vocabulary (3 points)**\n",
    "\n",
    "In this section, the task is to create a visual vocabulary by clustering feature descriptors extracted from the images using K-Means. Each cluster center in this vocabulary will represent a visual word. Use the two different extraction techniques you implemented to extract descriptors from a subset of training images that includes all categories, and then apply K-Means clustering to build the vocabulary. The number of clusters is fixed at 1000, but you can experiment with different values when you are tuning the hyperparameters in section 9.\n",
    "\n",
    "To examine the effect of different amounts of training data, build separate visual vocabularies using 30%, 40%, and 50% subsets of the training images. For faster clustering, the `faiss` library can be used, as it provides an efficient implementation of K-Means. Then, visualize the first 10 clusters for each feature extraction technique and each subset size using PCA to reduce the dimensions to 2D.\n",
    "\n",
    "**Hints:**\n",
    "1. Begin by debugging the code with a small number of input images to ensure it functions correctly before running it on larger datasets.\n",
    "2. If the `faiss` library is not available, K-Means clustering can also be performed using the `sklearn` or `scipy` libraries.\n",
    "3. For visualization, use PCA from `sklearn.decomposition` to reduce the high-dimensional descriptors to 2D. Display up to 10 clusters in the scatter plot to maintain clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply Kmeans clustering and form visual dictionaries\n",
    "def build_visual_vocabulary(descriptors, num_clusters=1000, mode='faiss'):\n",
    "\n",
    "    if mode == 'faiss':\n",
    "        kmeans = faiss.Kmeans(descriptors.shape[1], num_clusters, niter=20)\n",
    "        kmeans.train(descriptors)\n",
    "    elif mode == 'sklearn':\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "        kmeans.fit(descriptors)\n",
    "\n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptors_and_clusters(images, num_clusters=1000, mode='faiss'):\n",
    "    \"\"\" Extracts descriptors and clusters from the given images \n",
    "    \n",
    "    Args:\n",
    "        images: The images to extract descriptors and clusters from\n",
    "        num_clusters: The number of clusters to use for the visual vocabulary\n",
    "        mode: The mode to use for clustering ('faiss' or 'sklearn')\n",
    "        \n",
    "    Returns: \n",
    "        sift_descriptors: The SIFT descriptors extracted from the images\n",
    "        orb_descriptors: The ORB descriptors extracted from the images\n",
    "        sift_clusters: The SIFT clusters formed from the descriptors\n",
    "        orb_clusters: The ORB clusters formed from the descriptors\n",
    "    \n",
    "    \"\"\"\n",
    "    sift_descriptors = []\n",
    "    orb_descriptors = []\n",
    "\n",
    "    for img in images:\n",
    "\n",
    "        img = cv2.resize(fix_tensor(img), (128, 128))\n",
    "        _, img_sift_descriptors = get_image_keypoints_and_descriptors(img, method='sift')\n",
    "        _, img_orb_descriptors = get_image_keypoints_and_descriptors(img, method='orb')\n",
    "\n",
    "        if img_sift_descriptors is not None:\n",
    "            sift_descriptors.append(img_sift_descriptors)\n",
    "\n",
    "        if img_orb_descriptors is not None:\n",
    "            orb_descriptors.append(img_orb_descriptors)\n",
    "            \n",
    "    sift_descriptors = np.vstack(sift_descriptors)\n",
    "    orb_descriptors = np.vstack(orb_descriptors)\n",
    "\n",
    "    sift_clusters = build_visual_vocabulary(sift_descriptors, num_clusters, mode)\n",
    "    orb_clusters = build_visual_vocabulary(orb_descriptors, num_clusters, mode)\n",
    "    \n",
    "    return sift_descriptors, orb_descriptors, sift_clusters, orb_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize clusters using PCA\n",
    "def visualize_clusters(descriptors_list, kmeans_list, percentage, num_clusters=10, mode='faiss'):\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "\n",
    "    for n, descriptor in enumerate(descriptors_list):\n",
    "\n",
    "        reduced_descriptors = pca.fit_transform(descriptor)\n",
    "\n",
    "        for i in range(num_clusters):\n",
    "\n",
    "            if mode == 'faiss':\n",
    "                cluster_points = reduced_descriptors[kmeans_list[n].index.search(descriptor, 1)[1].flatten() == i]\n",
    "                cluster_centers = pca.transform(kmeans_list[n].centroids)\n",
    "            elif mode == 'sklearn':\n",
    "                cluster_points = reduced_descriptors[kmeans_list[n].labels_ == i]\n",
    "                cluster_centers = pca.transform(kmeans_list[n].cluster_centers_)\n",
    "\n",
    "            axes[n].scatter(cluster_points[:, 0], cluster_points[:, 1], label=f\"Cluster {i + 1}\", alpha=0.5)\n",
    "            axes[n].scatter(cluster_centers[i, 0], cluster_centers[i, 1], color='red', marker='x', s=50, linewidths=1)\n",
    "\n",
    "        axes[n].scatter([], [], color='red', marker='x', s=50, linewidths=1, label='Cluster Center')\n",
    "        axes[n].set_title(f\"{'SIFT' if n == 0 else 'ORB'} Clusters\")\n",
    "        axes[n].grid(True)\n",
    "\n",
    "        if n == 1:\n",
    "            axes[n].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    fig.suptitle(f\"Visualization of {num_clusters} Clusters: {percentage * 100}% of Training Data\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_mode = 'faiss'\n",
    "clusters_to_show = 10\n",
    "\n",
    "train_images_30_bow, train_images_70_svm, train_labels_30_bow, train_labels_70_svm = train_test_split(train_images, train_labels, test_size=0.7, random_state=42)\n",
    "train_images_40_bow, train_images_60_svm, train_labels_40_bow, train_labels_60_svm = train_test_split(train_images, train_labels, test_size=0.6, random_state=42)\n",
    "train_images_50_bow, train_images_50_svm, train_labels_50_bow, train_labels_50_svm = train_test_split(train_images, train_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "sift_descriptors_30, orb_descriptors_30, sift_clusters_30, orb_clusters_30 = get_descriptors_and_clusters(train_images_30_bow, num_of_visual_words, cluster_mode)\n",
    "sift_descriptors_40, orb_descriptors_40, sift_clusters_40, orb_clusters_40 = get_descriptors_and_clusters(train_images_40_bow, num_of_visual_words, cluster_mode)\n",
    "sift_descriptors_50, orb_descriptors_50, sift_clusters_50, orb_clusters_50 = get_descriptors_and_clusters(train_images_50_bow, num_of_visual_words, cluster_mode)\n",
    "\n",
    "visualize_clusters([sift_descriptors_30, orb_descriptors_30], [sift_clusters_30, orb_clusters_30], 0.3, clusters_to_show, cluster_mode)\n",
    "visualize_clusters([sift_descriptors_40, orb_descriptors_40], [sift_clusters_40, orb_clusters_40], 0.4, clusters_to_show, cluster_mode)\n",
    "visualize_clusters([sift_descriptors_50, orb_descriptors_50], [sift_clusters_50, orb_clusters_50], 0.5, clusters_to_show, cluster_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-4\"></a>\n",
    "### **Section 4: Encoding Train Image Features (3 points)**\n",
    "\n",
    "In this section, the task is to encode image features using the visual vocabulary created earlier. Each image will be represented as a histogram of visual words, reflecting the frequency of each visual word in the image. This representation will allow for comparing images based on their visual content.\n",
    "\n",
    "To encode an image, identify the nearest visual word (cluster center) for each feature descriptor extracted from the image. Construct a histogram that counts the occurrences of each visual word within the image. The final output will be a collection of histograms, one for each image, where each histogram serves as the feature representation of that image. Once again,  Use the two different extraction techniques you implemented to extract descriptors from the images. Then, encode the images using the visual vocabulary created in the previous step.\n",
    "\n",
    "**Hint:** Utilize the `faiss` library for efficient nearest neighbor search when assigning each descriptor to the nearest cluster center in the visual dictionary. If `faiss` is not available, consider using other libraries, such as `scikit-learn`, for this step. Once the histograms are obtained, they will be used for further tasks, such as training a classifier. For now, perform the encoding only for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def images_to_histograms(images, clusters, method, num_clusters, mode='faiss'):\n",
    "    \"\"\"\n",
    "    Encode image features using a visual vocabulary and represent them as histograms of visual words, reflecting the frequency of each visual word in the image.    \n",
    "    \n",
    "    Args:\n",
    "        images: A list of images\n",
    "        method: The feature extraction technique to use ('sift' or 'orb')\n",
    "        clusters: The clusters in which the nearest neighbour search will be performed\n",
    "        num_clusters: The number of clusters\n",
    "        mode: The mode to use for nearest neighbour search ('faiss', and if not available 'scikit-learn')\n",
    "        \n",
    "    Returns:\n",
    "        histogram_list: A list of histograms of visual words for each image.\n",
    "    \"\"\"\n",
    "    histogram_list = []\n",
    "\n",
    "    for image in images:   \n",
    "        \n",
    "        image = cv2.resize(fix_tensor(image), (128, 128))\n",
    "        # Obtain the descriptors for the image using the given method\n",
    "        _, descriptors = get_image_keypoints_and_descriptors(image, method=method)\n",
    "        \n",
    "        histogram = np.zeros(num_clusters)\n",
    "        if descriptors is None:\n",
    "            histogram_list.append(histogram)\n",
    "            continue\n",
    "\n",
    "        # Find the nearest cluster for each descriptor given the mode\n",
    "        if mode == 'faiss':\n",
    "            _, nearest_word = clusters.index.search(descriptors, 1)  \n",
    "            nearest_word = nearest_word.flatten()\n",
    "        elif mode == 'sklearn':\n",
    "            nearest_word = clusters.predict(descriptors)  \n",
    "            \n",
    "        # Update the histogram with the frequency of each visual word\n",
    "        for word in nearest_word:\n",
    "            histogram[word] += 1\n",
    "\n",
    "        # Normalize the histogram to compare images with different numbers of keypoints\n",
    "        histogram = normalize(histogram.reshape(1, -1), norm='l2').flatten()        \n",
    "        \n",
    "        histogram_list.append(histogram)\n",
    "\n",
    "    return np.array(histogram_list)\n",
    "\n",
    "print(\"Encode training images features using the SIFT technique\")\n",
    "train_histograms_sift = images_to_histograms(train_images, sift_clusters_50, 'sift', num_of_visual_words, cluster_mode)\n",
    "\n",
    "print(\"Encode training images features using the ORB technique\")\n",
    "train_histograms_orb = images_to_histograms(train_images, orb_clusters_50, 'orb', num_of_visual_words, cluster_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-5\"></a>\n",
    "### **Section 5: Visualizing the Bag of Visual Words for Each Class (3 points)**\n",
    "\n",
    "In this section, the task is to visualize the Bag of Visual Words for each class using the histograms generated in the previous step. The goal is to plot the mean histogram of visual words for each class, showing the distribution of visual words across the different categories in the training set.\n",
    "\n",
    "Use the two different extraction techniques you implemented for this visualization. For each technique, calculate the mean histogram for each class and create a bar plot to display these histograms. Ensure that the plots are labeled clearly with the class names and feature descriptor types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def visualize_bow_per_class(histograms, labels, num_classes, num_clusters, class_names, method):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "        histograms: A list of histograms of visual words for each image\n",
    "        labels: A list of labels for each image in the histogram\n",
    "        num_classes: The number of classes\n",
    "        num_clusters: The number of clusters\n",
    "        class_names: A list of class names\n",
    "        method: The feature extraction technique used ('sift' or 'orb')\n",
    "\n",
    "    \"\"\"\n",
    "    histogram_dict = defaultdict(lambda: np.zeros(num_clusters))     \n",
    "    counts_dict = defaultdict(int)\n",
    "\n",
    "    # Sum the histograms for each class\n",
    "    for histogram, label in zip(histograms, labels):\n",
    "        histogram_dict[label.item()] += histogram\n",
    "        counts_dict[label.item()] += 1\n",
    "\n",
    "    # Calculate the mean histogram for each class\n",
    "    mean_histogram_dict = {label: histogram_dict[label] / counts_dict[label]\n",
    "                       for label in range(num_classes)}\n",
    "    \n",
    "    # Plot the mean histograms for each class\n",
    "    fig, axes = plt.subplots(num_classes, 1, figsize=(15, 20))\n",
    "\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        axes[i].bar(range(num_clusters), mean_histogram_dict[i])\n",
    "        axes[i].set_title(f\"Class: {class_name}, Method: {method}\")\n",
    "        axes[i].set_ylabel(\"Mean Frequency\")\n",
    "        axes[i].set_xlabel(\"Visual Words (Clusters)\")\n",
    "    \n",
    "    # axes[-1].set_xlabel(\"Visual Words (Clusters)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the BoW histograms using SIFT\n",
    "visualize_bow_per_class(train_histograms_sift, train_labels_50_bow, num_classes, num_of_visual_words, class_names, method='SIFT')\n",
    "# Visualize the BoW histograms using ORB\n",
    "visualize_bow_per_class(train_histograms_orb, train_labels_50_bow, num_classes, num_of_visual_words, class_names, method='ORB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-6\"></a>\n",
    "### **Section 6: Encoding Test Image Features (0 points)**\n",
    "\n",
    "In this section, the task is to encode the test image features using the visual vocabulary created from the training set. Similar to the previous encoding step, each test image will be represented as a histogram of visual words, which will then be used for evaluating classification performance.\n",
    "\n",
    "Use the same two feature extraction techniques you selected earlier. Extract keypoints and descriptors for the test images, then encode these images using the visual vocabulary. This will allow you to compare the encoded features of test images against those of the training set.\n",
    "\n",
    "**Hint:** Reuse the functions developed earlier for extracting keypoints, descriptors, and encoding images. Ensure that you use the visual vocabulary constructed with the training images for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Encode test images features using the SIFT technique\")\n",
    "test_histograms_sift = images_to_histograms(test_images, sift_clusters_50, 'sift', num_of_visual_words, cluster_mode)\n",
    "\n",
    "print(\"Encode test images features using the ORB technique\")\n",
    "test_histograms_orb = images_to_histograms(test_images, orb_clusters_50, 'orb', num_of_visual_words, cluster_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-7\"></a>\n",
    "### **Section 7: Training the Classifiers (5 points)**\n",
    "\n",
    "In this section, the task is to create two one-vs-rest (OvR) SVM classifiers using the 50% of the training data that was **not** used for creating the visual dictionary. This ensures that the classifiers are trained on a different subset of data, providing a more robust evaluation of the visual vocabulary's effectiveness.\n",
    "\n",
    "For each of the two selected feature extraction techniques, create one-vs-rest classifiers for all classes. For now, use default parameter values when training the classifiers; you will experiment with different hyperparameters in later steps.\n",
    "\n",
    "**Note:** Training an OvR classifier can take around 5 to 7 minutes. Therefore, it's advisable to first test your code with a smaller subset of the training data to verify that your implementation works correctly before running it on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Encode training images features using the SIFT technique\")\n",
    "train_histograms_sift_svm = images_to_histograms(train_images_50_svm, sift_clusters_50, 'sift', num_of_visual_words, cluster_mode)\n",
    "\n",
    "print(\"Encode training images features using the ORB technique\")\n",
    "train_histograms_orb_svm = images_to_histograms(train_images_50_svm, orb_clusters_50, 'orb', num_of_visual_words, cluster_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train (OvR) SVM on SIFT features\")\n",
    "ovr_svm_sift = OneVsRestClassifier(SVC(kernel='linear', random_state=42))\n",
    "ovr_svm_sift.fit(train_histograms_sift_svm, train_labels_50_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Train (OvR) SVM on ORB features\")\n",
    "ovr_svm_orb = OneVsRestClassifier(SVC(kernel='linear', random_state=42))\n",
    "ovr_svm_orb.fit(train_histograms_sift_svm, train_labels_50_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-8\"></a>\n",
    "### **Section 8: Evaluating the Classifiers (12 points)**\n",
    "\n",
    "In this section, you will evaluate the performance of your one-vs-rest (OvR) SVM classifiers on the test data. The goal is to classify each test image using each binary classifier and rank the images based on the classification scores, resulting in a ranked list of images for each class. Ideally, images belonging to the target class should appear at the top of the respective list. To conduct this evaluation, use the test image histograms generated earlier for the two selected feature extraction techniques. Classify each test image with each classifier, rank them based on their confidence scores, and then compute the Mean Average Precision (mAP) across all classes. The mAP for a single class $c$ is defined as:\n",
    "\n",
    "$\n",
    "\\text{mAP}_c = \\frac{1}{m_c} \\sum_{i=1}^{n} \\frac{f_c(x_i)}{i}\n",
    "$\n",
    "\n",
    "where:\n",
    "- $n$ is the total number of images ($n = 50 \\times 5 = 250$),\n",
    "- $m_c$ is the number of images of class $c$ ($m_c = 50$),\n",
    "- $x_i$ is the $i^{th}$ image in the ranked list $X = \\{ x_1, x_2, \\dots, x_n \\}$,\n",
    "- $f_c$ is a function that returns the number of images of class $c$ in the first $i$ images if $x_i$ is of class $c$, and 0 otherwise.\n",
    "\n",
    "For instance, if you are retrieving images of class \"R\" and the sequence of ranked images is $[R, R, T, R, T, T, R, T]$, then $n = 8$, $m_c = 4$, and:\n",
    "\n",
    "$\n",
    "AP = \\frac{1}{4} \\left( \\frac{1}{1} + \\frac{2}{2} + \\frac{0}{3} + \\frac{3}{4} + \\frac{0}{5} + \\frac{0}{6} + \\frac{4}{7} + \\frac{0}{8} \\right).\n",
    "$\n",
    "\n",
    "In addition to the quantitative analysis, perform a qualitative analysis by visualizing the top-5 and bottom-5 ranked test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_images_class(images, ranked_i, scores, c):\n",
    "    \"\"\" Visualises the top-5 and bottom-5 ranked images for a certain class   \n",
    "\n",
    "    Args:\n",
    "        images: A list of test images\n",
    "        ranked_i: A list of indices of the images ranked by confidence score for a class\n",
    "        scores: A list of confidence scores of a certain class for each image\n",
    "        c: The class to visualise the images for\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Class {c}: Top-5 Images:\")\n",
    "    top5_i = ranked_i[:5]\n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20, 10))\n",
    "    \n",
    "    for i, image_i in enumerate(top5_i):\n",
    "        image = cv2.resize(fix_tensor(images[image_i]), (128, 128))\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].axis('off') \n",
    "        axes[i].set_title(f'Confidence score: {scores[image_i]:.2f}')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Class {c}: Bottom-5 Images:\")\n",
    "    bottom5_i = ranked_i[-5:]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20, 10))\n",
    "    \n",
    "    for i, image_i in enumerate(bottom5_i):\n",
    "        image = cv2.resize(fix_tensor(images[image_i]), (128, 128))\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].axis('off')  \n",
    "        axes[i].set_title(f'Confidence score: {scores[image_i]:.2f}')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_map(images, scores, labels, num_classes, class_names, visualize=False):\n",
    "    \"\"\" Calculates the mean average precision over all classes\n",
    "    \n",
    "    Args: \n",
    "        images: A list of test images\n",
    "        scores: A list of confidence scores for each image and class\n",
    "        labels: A list of true labels for each image\n",
    "        num_classes: The number of classes\n",
    "        class_names: A list of class names\n",
    "\n",
    "    Returns:\n",
    "        map: The mean average precision over all classes\n",
    "    \"\"\"\n",
    "    ap_class = []\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "\n",
    "        # Obtain the confidence scores for the class\n",
    "        class_scores = scores[:, c]\n",
    "        true_labels = (labels == c).long() # 1 if the label is the class, 0 otherwise\n",
    "        \n",
    "        # Rank the images by confidence score\n",
    "        ranked_class_i = np.argsort(-class_scores)\n",
    "        ranked_true_labels = true_labels[ranked_class_i]\n",
    "\n",
    "        # Calculate the average precision for the class\n",
    "        sum_correct = 0\n",
    "        ap = 0\n",
    "        for i, label in enumerate(ranked_true_labels):\n",
    "            if label == 1:\n",
    "                sum_correct += 1\n",
    "                ap += sum_correct / (i + 1)\n",
    "        m_c = torch.sum(ranked_true_labels).item()\n",
    "        ap /= m_c\n",
    "        ap_class.append(ap)\n",
    "\n",
    "        if visualize is True:\n",
    "            # Visualize the top-5 and bottom-5 ranked images for the class\n",
    "            visualize_images_class(images, ranked_class_i, class_scores, class_names[c])\n",
    "\n",
    "    map = np.mean(ap_class)\n",
    "    return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_sift = ovr_svm_sift.decision_function(test_histograms_sift)\n",
    "scores_orb = ovr_svm_orb.decision_function(test_histograms_orb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_sift = calculate_map(test_images, scores_sift, test_labels, num_classes, class_names, True)\n",
    "print(f\"The Mean Average Precision (MAP) across all classes using SIFT: {map_shift}\")\n",
    "\n",
    "map_orb = calculate_map(test_images, scores_orb, test_labels, num_classes, class_names, True)\n",
    "print(f\"The Mean Average Precision (MAP) across all classes using ORB: {map_orb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-9\"></a>\n",
    "### **Section 9: Hyperparameter Search (16 points)**\n",
    "\n",
    "In this section, the task is to perform an extensive hyperparameter search to optimize the performance of your classifiers. You will experiment with various parameters, including the number of visual words (e.g., 500, 1000, 1500), different training subset sizes (e.g., 30%, 40%, 50%), SVM parameters (e.g., kernel types like 'linear' or 'rbf', regularization parameter $C$ values such as 0.1, 1, 10, and gamma settings like 'scale' or specific values such as 0.01, 0.001), and settings of the feature extractors (e.g., the number of keypoints or scale levels). Start by testing your code on the smallest subset to ensure it functions correctly before proceeding with a full hyperparameter search. Once validated, conduct the search using larger subsets and systematically iterate through the different parameter combinations, potentially using nested loops or grid search. Be sure to record the performance results for each combination to identify the best settings based on metrics like the Mean Average Precision (mAP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_histograms_sift_svm_30 = images_to_histograms(train_images, sift_clusters_30, 'sift', num_of_visual_words, cluster_mode)\n",
    "train_histograms_sift40 = images_to_histograms(train_images, sift_clusters_40, 'sift', num_of_visual_words, cluster_mode)\n",
    "train_histograms_sift50 = images_to_histograms(train_images, sift_clusters_50, 'sift', num_of_visual_words, cluster_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "parameters = {'num_of_visual_words': [500, 1000, 1500],\n",
    "                   'train_subset': [30, 40, 50],\n",
    "                   'kernel': ['linear', 'rbf'], \n",
    "                   'C': [0.1, 1, 10],\n",
    "                   'gamma': [0.01, 0.001, 'scale']}\n",
    "\n",
    "parameter_grid = ParameterGrid(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sift = []\n",
    "for params in tqdm(parameter_grid, desc=\"parameter\"):\n",
    "    if params['train_subset'] == 30:\n",
    "        images_bow = train_images_30_bow\n",
    "        images_svm = train_images_70_svm\n",
    "        labels_svm = train_labels_70_svm\n",
    "    elif params['train_subset'] == 40:\n",
    "        images_bow = train_images_40_bow\n",
    "        images_svm = train_images_60_svm\n",
    "        labels_svm = train_labels_60_svm\n",
    "    elif params['train_subset'] == 50:\n",
    "        images_bow = train_images_50_bow\n",
    "        images_svm = train_images_50_svm\n",
    "        labels_svm = train_labels_50_svm\n",
    "\n",
    "    sift_descriptors, _, sift_clusters, _ = get_descriptors_and_clusters(images_bow, params['num_of_visual_words'], cluster_mode)\n",
    "    \n",
    "    train_histograms_sift_svm = images_to_histograms(images_svm, sift_clusters, 'sift', params['num_of_visual_words'], cluster_mode)\n",
    "    test_histograms_sift_svm = images_to_histograms(test_images, sift_clusters, 'sift', params['num_of_visual_words'], cluster_mode)\n",
    "    \n",
    "    sift_svc = OneVsRestClassifier(SVC(kernel=params[\"kernel\"], C=params[\"C\"], gamma=params[\"gamma\"], random_state=42))\n",
    "    sift_svc.fit(train_histograms_sift_svm, labels_svm)\n",
    "    scores_sift = sift_svc.decision_function(test_histograms_sift_svm)\n",
    "    map_score = calculate_map(test_images, scores_sift, test_labels, num_classes, class_names, False)\n",
    "\n",
    "    results_sift.append({'params': params, 'mAP': map_score})\n",
    "    print(f\"Parameters: {params}, mAP: {map_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_orb = []\n",
    "for params in tqdm(parameter_grid, desc=\"parameter\"):\n",
    "    if params['train_subset'] == 30:\n",
    "        images_bow = train_images_30_bow\n",
    "        images_svm = train_images_70_svm\n",
    "        labels_svm = train_labels_70_svm\n",
    "    elif params['train_subset'] == 40:\n",
    "        images_bow = train_images_40_bow\n",
    "        images_svm = train_images_60_svm\n",
    "        labels_svm = train_labels_60_svm\n",
    "    elif params['train_subset'] == 50:\n",
    "        images_bow = train_images_50_bow\n",
    "        images_svm = train_images_50_svm\n",
    "        labels_svm = train_labels_50_svm\n",
    "\n",
    "    _, orb_descriptors, _, orb_clusters = get_descriptors_and_clusters(images_bow, params['num_of_visual_words'], cluster_mode)\n",
    "    \n",
    "    train_histograms_orb_svm = images_to_histograms(images_svm, orb_clusters, 'orb', params['num_of_visual_words'], cluster_mode)\n",
    "    test_histograms_orb_svm = images_to_histograms(test_images, orb_clusters, 'orb', params['num_of_visual_words'], cluster_mode)\n",
    "    \n",
    "    orb_svc = OneVsRestClassifier(SVC(kernel=params[\"kernel\"], C=params[\"C\"], gamma=params[\"gamma\"], random_state=42))\n",
    "    orb_svc.fit(train_histograms_orb_svm, labels_svm)\n",
    "    scores_orb = orb_svc.decision_function(test_histograms_orb_svm)\n",
    "    map_score = calculate_map(test_images, scores_orb, test_labels, num_classes, class_names, False)\n",
    "\n",
    "    results_orb.append({'params': params, 'mAP': map_score})\n",
    "    print(f\"Parameters: {params}, mAP: {map_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-10\"></a>\n",
    "### **Section 10: Using CLIP for Image Classification (5 points)**\n",
    "\n",
    "**<span style=\"color:red\">⚠️ NOTE: This section should NOT be included in the report. It is only meant to be completed in the code cells. The purpose of this task is to introduce you to a more state-of-the-art model (CLIP) compared to Bag of Visual Words (BoVW). Vision Transformers (ViT) will be covered in more detail in the Deep Learning 1 course next period!</span>**\n",
    "\n",
    "In this section, you will use a pre-trained CLIP model for image classification. CLIP (Contrastive Language-Image Pretraining) is a vision-language transformer model trained on a large dataset of images and text. It consists of two main components: a Vision Transformer (ViT) and a text Transformer. The ViT encodes images by dividing them into patches (tokens), flattening each patch into a vector, and passing them through a sequence of Transformer layers to produce an encoded representation of the image.\n",
    "\n",
    "For this task, you will use the visual transformer component of CLIP to extract encoded representations of the input images. While this is not the typical way to use CLIP (which involves encoding both images and text for similarity comparison), it provides an interesting application of this state-of-the-art model for image classification.\n",
    "\n",
    "**To Install CLIP:**\n",
    "```python\n",
    "pip install git+https://github.com/openai/CLIP.git\n",
    "```\n",
    "\n",
    "**Additional Reading (if you're interested):**\n",
    "- [OpenAI CLIP Overview](https://openai.com/clip)\n",
    "- [Vision Transformer (ViT) Paper](https://arxiv.org/abs/2010.11929)\n",
    "- [Tutorial on Vision Transformers](https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html)\n",
    "- [UvA's Deep Learning Introduction to ViTs](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create DataLoaders for both the training and test datasets by filtering the CIFAR-10 dataset to include only the selected classes: frog, automobile, bird, cat, and deer. Use a batch size of 16 for both DataLoaders, and resize the images to 224x224 to match the input size requirements for CLIP. Remember to normalize the images using the appropriate mean and standard deviation. Use a training set size of 1000 images per class and a test set size of 200 images per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Define the class indices for the 5 selected classes: frog, automobile, bird, cat, and deer\n",
    "selected_classes = [6, 1, 2, 3, 4]  # 6: frog, 1: automobile, 2: bird, 3: cat, 4: deer\n",
    "class_to_label = {orig_class: new_label for new_label, orig_class in enumerate(selected_classes)}\n",
    "\n",
    "# Load the CIFAR-10 training set\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Filter and remap training set\n",
    "filtered_train_set, train_mapped_labels = filter_dataset(train_set, 1000, selected_classes, class_to_label)\n",
    "\n",
    "# Load the CIFAR-10 test set\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Filter and remap test set\n",
    "filtered_test_set, test_mapped_labels = filter_dataset(test_set, 200, selected_classes, class_to_label)\n",
    "\n",
    "# Create data loaders for the filtered datasets\n",
    "train_data_loader = DataLoader(filtered_train_set, batch_size=16, shuffle=False)\n",
    "test_data_loader = DataLoader(filtered_test_set, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load a pre-trained CLIP model, which is a vision-language transformer designed to predict the text that describes an image and vice versa. The model consists of two components: a Vision Transformer (ViT) for encoding images and a text Transformer for encoding text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "# Setup the model and the preprocessor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the pre-trained CLIP model\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the visual tokens using the CLIP model, start by initializing two empty lists: one for storing the image features and another for the labels. Use `tqdm` to create a progress bar that tracks the extraction process over the DataLoader. Iterate through the DataLoader, extracting images and labels. Disable gradient computation, and then encode the images with `model.encode_image(images)`. Append the encoded features and labels to their respective lists. Remember, the output from the model will have the shape `(batch_size, 512)` due to batched processing, if your are using a single you should reshape the output to `(512,)` to remove the batch dimension.\n",
    "\n",
    "In this example, we will use the class token as the visual representation of the image. The class token is a 512-dimensional vector that represents the image. We will use this vector to train a classifier to classify the images.\n",
    "\n",
    "**Note:** The CLIP model is quite large and may take some time to extract features from the images. You can use the `tqdm` library to create a progress bar that shows the extraction progress. It is recommended to test the code with a smaller subset of images to ensure it functions correctly before running it on the full dataset. With the default batch size of 16 and a training set size of 1000 images, the extraction process may take a 10-20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "image_features_list = []\n",
    "target_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(train_data_loader, desc=\"Extracting features\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        image_features = model.encode_image(images)\n",
    "\n",
    "        image_features_list.append(image_features)\n",
    "        target_labels_list.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the (batched) visual tokens extracted from the images. The resulting shape will be (number_of_images, 512).\n",
    "stacked_image_features = torch.cat(image_features_list)\n",
    "stacked_target_labels = torch.cat(target_labels_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a classifier using the visual tokens, start by initializing an SVM classifier using `SVC()` from `scikit-learn`. You can play around with different hyperparameters such as kernel type, regularization parameter, and gamma to find the best configuration. Finally, use the `fit` method to train the classifier on the visual image features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm_classifier.fit(stacked_image_features.cpu().detach().numpy(), stacked_target_labels.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the classifier, start by extracting the visual tokens from the test images using the same method applied to the training set. Loop through the test DataLoader, encode each batch of images using the model (e.g., `model.encode_image(images)`), and store the results in separate lists for the features and labels. After extracting all the features, stack them into a single tensor for both the features and labels. This process will prepare the test data for use in evaluating the classifier's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_features_list = []\n",
    "test_target_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_data_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        image_features = model.encode_image(images)\n",
    "\n",
    "        test_image_features_list.append(image_features)\n",
    "        test_target_labels_list.append(labels)\n",
    "\n",
    "stacked_test_image_features = torch.cat(test_image_features_list)\n",
    "stacked_test_target_labels = torch.cat(test_target_labels_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of your classifier, use the test set features you extracted earlier. First, generate predictions for the test set by passing the stacked test features into your trained classifier's `predict` method. Next, use the `classification_report` function from `sklearn.metrics` to create a detailed report that includes metrics such as precision, recall, and F1-score for each class. Finally, print the report to analyze how well your classifier performs across the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predicted_labels = svm_classifier.predict(stacked_test_image_features.cpu().detach().numpy())\n",
    "\n",
    "print(classification_report(stacked_test_target_labels.cpu().detach().numpy(), predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-x\"></a>\n",
    "### **Section X: Individual Contribution Report *(Mandatory)***\n",
    "\n",
    "Because we want each student to contribute fairly to the submitted work, we ask you to fill out the textcells below. Write down your contribution to each of the assignment components in percentages. Naturally, percentages for one particular component should add up to 100% (e.g. 30% - 30% - 40%). No further explanation has to be given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name | Contribution on Research | Contribution on Programming | Contribution on Writing |\n",
    "| -------- | ------- | ------- | ------- |\n",
    "| Jose | 33 % | 33 % | 33 % |\n",
    "| Lisanne | 33 % | 33 % | 33 % |\n",
    "| Julio | 33 % | 33 % | 33 % |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - End of Notebook -"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv1_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
